import pandas as pd
import numpy as np
import matplotlib
import os
from datetime import datetime

matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.impute import KNNImputer, SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline as SkPipeline
from sklearn.preprocessing import OneHotEncoder
import joblib
import warnings

warnings.filterwarnings('ignore')


class HealthDataPipeline:
    """
    SaÄŸlÄ±k verisi iÃ§in kapsamlÄ± veri iÅŸleme ve analiz pipeline'Ä±
    """

    def __init__(self, data_path=None):
        """
        Pipeline'Ä± baÅŸlat

        Args:
            data_path (str): Veri dosyasÄ±nÄ±n yolu
        """
        self.data_path = data_path
        self.raw_data = None
        self.processed_data = None
        self.scaler = StandardScaler()
        self.label_encoders = {}
        self.target_label_encoder = None
        self.pipeline_steps = []
        self.output_dir = os.path.join(os.getcwd(), 'outputs')
        os.makedirs(self.output_dir, exist_ok=True)

        # GÃ¶rselleÅŸtirme ayarlarÄ±
        sns.set_theme(style="whitegrid")
        plt.rcParams['figure.figsize'] = [10, 6]

    def load_data(self, data_path=None):
        """
        Veri dosyasÄ±nÄ± yÃ¼kle

        Args:
            data_path (str): Veri dosyasÄ±nÄ±n yolu
        """
        if data_path:
            self.data_path = data_path

        try:
            if self.data_path.endswith('.xlsx'):
                self.raw_data = pd.read_excel(self.data_path)
            elif self.data_path.endswith('.csv'):
                self.raw_data = pd.read_csv(self.data_path)
            else:
                raise ValueError("Desteklenen dosya formatlarÄ±: .xlsx, .csv")

            print(f"Veri baÅŸarÄ±yla yÃ¼klendi. Boyut: {self.raw_data.shape}")
            self.pipeline_steps.append("Veri yÃ¼klendi")
            return self.raw_data

        except Exception as e:
            print(f"Veri yÃ¼kleme hatasÄ±: {e}")
            return None

    def explore_data(self):
        """
        Veri keÅŸfi yap
        """
        if self.raw_data is None:
            print("Ã–nce veri yÃ¼kleyin!")
            return

        print("=== VERÄ° KEÅFÄ° ===")
        print(f"Veri boyutu: {self.raw_data.shape}")
        print(f"Toplam bellek kullanÄ±mÄ±: {self.raw_data.memory_usage(deep=True).sum() / 1024 ** 2:.2f} MB")

        print("\n=== VERÄ° TÄ°PLERÄ° ===")
        print(self.raw_data.dtypes)

        print("\n=== EKSÄ°K VERÄ° ANALÄ°ZÄ° ===")
        missing_data = self.raw_data.isnull().sum()
        missing_percent = (missing_data / len(self.raw_data)) * 100
        missing_df = pd.DataFrame({
            'Eksik Veri SayÄ±sÄ±': missing_data,
            'Oran (%)': missing_percent
        })
        print(missing_df[missing_df['Eksik Veri SayÄ±sÄ±'] > 0])

        print("\n=== SAYISAL SÃœTUNLAR Ä°Ã‡Ä°N Ä°STATÄ°STÄ°KLER ===")
        numeric_cols = self.raw_data.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            print(self.raw_data[numeric_cols].describe())

        self.pipeline_steps.append("Veri keÅŸfi tamamlandÄ±")

    def visualize_data(self, max_pairplot_features=5):
        """
        EDA gÃ¶rselleÅŸtirmeleri Ã¼ret ve kaydet (histogramlar, korelasyon Ä±sÄ± haritasÄ±, sÄ±nÄ±rlÄ± pairplot)
        """
        if self.raw_data is None:
            print("Ã–nce veri yÃ¼kleyin!")
            return

        print("=== EDA GÃ–RSELLEÅTÄ°RMELERÄ° OLUÅTURULUYOR ===")

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # Histogramlar (sayÄ±sal sÃ¼tunlar)
        numeric_cols = self.raw_data.select_dtypes(include=[np.number]).columns.tolist()
        if numeric_cols:
            try:
                ax = self.raw_data[numeric_cols].hist(bins=20, figsize=(14, 10))
                plt.suptitle('SayÄ±sal SÃ¼tunlar - Histogramlar')
                hist_path = os.path.join(self.output_dir, f'histograms_{timestamp}.png')
                plt.tight_layout()
                plt.savefig(hist_path)
                plt.close()
                print(f"Histogram gÃ¶rseli kaydedildi: {os.path.abspath(hist_path)}")
            except Exception as e:
                print(f"Histogram oluÅŸturulamadÄ±: {e}")

        # Korelasyon Ä±sÄ± haritasÄ±
        if numeric_cols and len(numeric_cols) >= 2:
            try:
                corr = self.raw_data[numeric_cols].corr(numeric_only=True)
                plt.figure(figsize=(12, 8))
                sns.heatmap(corr, cmap='coolwarm', center=0)
                plt.title('Korelasyon IsÄ± HaritasÄ±')
                corr_path = os.path.join(self.output_dir, f'correlation_heatmap_{timestamp}.png')
                plt.tight_layout()
                plt.savefig(corr_path)
                plt.close()
                print(f"Korelasyon Ä±sÄ± haritasÄ± kaydedildi: {os.path.abspath(corr_path)}")
            except Exception as e:
                print(f"Korelasyon Ä±sÄ± haritasÄ± oluÅŸturulamadÄ±: {e}")

        # Pairplot (sÄ±nÄ±rlÄ± sayÄ±da Ã¶zellik)
        if numeric_cols:
            try:
                selected = numeric_cols[:max_pairplot_features]
                sns.pairplot(self.raw_data[selected], corner=True)
                pairplot_path = os.path.join(self.output_dir, f'pairplot_{timestamp}.png')
                plt.savefig(pairplot_path)
                plt.close()
                print(f"Pairplot kaydedildi: {os.path.abspath(pairplot_path)}")
            except Exception as e:
                print(f"Pairplot oluÅŸturulamadÄ±: {e}")

        self.pipeline_steps.append("EDA gÃ¶rselleÅŸtirmeleri oluÅŸturuldu")

    def clean_data(self):
        """
        Veri temizliÄŸi yap
        """
        print("=== VERÄ° TEMÄ°ZLÄ°ÄÄ° BAÅLADI ===")

        # Kopya oluÅŸtur
        self.processed_data = self.raw_data.copy()

        # Eksik veri doldurma stratejileri
        self._fill_missing_values()

        # Veri tipi dÃ¶nÃ¼ÅŸÃ¼mleri
        self._convert_data_types()

        # Duplicate kayÄ±tlarÄ± temizle
        initial_rows = len(self.processed_data)
        self.processed_data = self.processed_data.drop_duplicates()
        final_rows = len(self.processed_data)

        if initial_rows != final_rows:
            print(f"Duplicate kayÄ±tlar temizlendi: {initial_rows - final_rows} kayÄ±t kaldÄ±rÄ±ldÄ±")

        print("Veri temizliÄŸi tamamlandÄ±!")
        self.pipeline_steps.append("Veri temizliÄŸi tamamlandÄ±")

    def _fill_missing_values(self):
        """
        Eksik verileri doldur
        """
        # Kategorik sÃ¼tunlar iÃ§in mode kullan
        categorical_cols = self.processed_data.select_dtypes(include=['object']).columns

        for col in categorical_cols:
            if self.processed_data[col].isnull().any():
                mode_value = self.processed_data[col].mode()[0]
                self.processed_data[col].fillna(mode_value, inplace=True)
                print(f"'{col}' sÃ¼tunundaki eksik veriler '{mode_value}' ile dolduruldu")

        # SayÄ±sal sÃ¼tunlar iÃ§in median kullan
        numeric_cols = self.processed_data.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if self.processed_data[col].isnull().any():
                median_value = self.processed_data[col].median()
                self.processed_data[col].fillna(median_value, inplace=True)
                print(f"'{col}' sÃ¼tunundaki eksik veriler median ({median_value}) ile dolduruldu")

    def _convert_data_types(self):
        """
        Veri tiplerini dÃ¶nÃ¼ÅŸtÃ¼r
        """
        # TedaviSuresi ve UygulamaSuresi sÃ¼tunlarÄ±nÄ± temizle ve sayÄ±ya Ã§evir
        if 'TedaviSuresi' in self.processed_data.columns:
            self.processed_data['TedaviSuresi'] = self.processed_data['TedaviSuresi'].astype(str).str.extract(
                r'(\d+)').astype(float)

        if 'UygulamaSuresi' in self.processed_data.columns:
            self.processed_data['UygulamaSuresi'] = self.processed_data['UygulamaSuresi'].astype(str).str.extract(
                r'(\d+)').astype(float)

    def feature_engineering(self, preserve_columns=None):
        """
        Ã–zellik mÃ¼hendisliÄŸi yap

        Args:
            preserve_columns (list): One-hot encoding'den korunacak sÃ¼tunlar
        """
        if self.processed_data is None:
            print("Ã–nce veri temizliÄŸi yapÄ±n!")
            return

        print("=== Ã–ZELLÄ°K MÃœHENDÄ°SLÄ°ÄÄ° BAÅLADI ===")

        # Korunacak sÃ¼tunlarÄ± belirt
        if preserve_columns is None:
            preserve_columns = []

        # Alerji sÃ¼tununu one-hot encoding ile dÃ¶nÃ¼ÅŸtÃ¼r
        if 'Alerji' in self.processed_data.columns:
            alerji_dummies = self.processed_data['Alerji'].str.get_dummies(sep=',')
            alerji_dummies = alerji_dummies.add_prefix('Alerji_')
            self.processed_data = pd.concat([self.processed_data, alerji_dummies], axis=1)
            self.processed_data.drop('Alerji', axis=1, inplace=True)
            print("Alerji sÃ¼tunu one-hot encoding ile dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼")

        # Kategorik sÃ¼tunlarÄ± one-hot encoding ile dÃ¶nÃ¼ÅŸtÃ¼r (korunacak sÃ¼tunlar hariÃ§)
        categorical_cols = self.processed_data.select_dtypes(include=['object']).columns
        categorical_cols = [col for col in categorical_cols if col not in preserve_columns]

        if len(categorical_cols) > 0:
            self.processed_data = pd.get_dummies(self.processed_data, columns=categorical_cols, drop_first=True)
            print(f"Kategorik sÃ¼tunlar one-hot encoding ile dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼: {list(categorical_cols)}")

        # Gereksiz sÃ¼tunlarÄ± kaldÄ±r
        cols_to_drop = ['HastaNo', 'Tanilar', 'TedaviAdi', 'UygulamaYerleri', 'KronikHastalik']
        existing_cols_to_drop = [col for col in cols_to_drop if col in self.processed_data.columns]

        if existing_cols_to_drop:
            self.processed_data.drop(columns=existing_cols_to_drop, inplace=True)
            print(f"Gereksiz sÃ¼tunlar kaldÄ±rÄ±ldÄ±: {existing_cols_to_drop}")

        print("Ã–zellik mÃ¼hendisliÄŸi tamamlandÄ±!")
        self.pipeline_steps.append("Ã–zellik mÃ¼hendisliÄŸi tamamlandÄ±")

    def scale_features(self):
        """
        SayÄ±sal Ã¶zellikleri Ã¶lÃ§eklendir
        """
        if self.processed_data is None:
            print("Ã–nce Ã¶zellik mÃ¼hendisliÄŸi yapÄ±n!")
            return

        print("=== Ã–ZELLÄ°K Ã–LÃ‡EKLENDÄ°RME BAÅLADI ===")

        # SayÄ±sal sÃ¼tunlarÄ± bul
        numeric_cols = self.processed_data.select_dtypes(include=[np.number]).columns

        if len(numeric_cols) > 0:
            # StandardScaler uygula
            self.processed_data[numeric_cols] = self.scaler.fit_transform(self.processed_data[numeric_cols])
            print(f"SayÄ±sal sÃ¼tunlar standartlaÅŸtÄ±rÄ±ldÄ±: {list(numeric_cols)}")

        self.pipeline_steps.append("Ã–zellik Ã¶lÃ§eklendirme tamamlandÄ±")

    def prepare_for_modeling(self, target_column=None):
        """
        Modelleme iÃ§in veriyi hazÄ±rla

        Args:
            target_column (str): Hedef deÄŸiÅŸken sÃ¼tunu
        """
        if self.processed_data is None:
            print("Ã–nce veri iÅŸleme adÄ±mlarÄ±nÄ± tamamlayÄ±n!")
            return None, None

        print("=== MODELLEME HAZIRLIÄI ===")

        # Hedef deÄŸiÅŸken belirtilmemiÅŸse, ilk sayÄ±sal sÃ¼tunu kullan
        if target_column is None:
            numeric_cols = self.processed_data.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                target_column = numeric_cols[0]
                print(f"Hedef deÄŸiÅŸken otomatik seÃ§ildi: {target_column}")

        if target_column not in self.processed_data.columns:
            print(f"Hedef sÃ¼tun bulunamadÄ±: {target_column}")
            return None, None

        # Hedef deÄŸiÅŸkeni ayÄ±r
        X = self.processed_data.drop(columns=[target_column])
        y = self.processed_data[target_column]

        print(f"Ã–zellik matrisi boyutu: {X.shape}")
        print(f"Hedef deÄŸiÅŸken boyutu: {y.shape}")

        self.pipeline_steps.append("Modelleme hazÄ±rlÄ±ÄŸÄ± tamamlandÄ±")
        return X, y

    def train_model(self, X, y, test_size=0.2, random_state=42, model_type='auto'):
        """
        Model eÄŸitimi yap

        Args:
            X: Ã–zellik matrisi
            y: Hedef deÄŸiÅŸken
            test_size: Test seti oranÄ±
            random_state: Rastgelelik iÃ§in seed
            model_type: Model tipi ('auto', 'classification', 'regression')
        """
        print("=== MODEL EÄÄ°TÄ°MÄ° BAÅLADI ===")

        # Veriyi eÄŸitim ve test setlerine ayÄ±r
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state
        )

        print(f"EÄŸitim seti boyutu: {X_train.shape}")
        print(f"Test seti boyutu: {X_test.shape}")

        # Hedef deÄŸiÅŸken tipini otomatik belirle
        if model_type == 'auto':
            if y.dtype in ['object', 'bool'] or len(y.unique()) < 20:
                model_type = 'classification'
                print("Hedef deÄŸiÅŸken tipi otomatik belirlendi: SÄ±nÄ±flandÄ±rma")
            else:
                model_type = 'regression'
                print("Hedef deÄŸiÅŸken tipi otomatik belirlendi: Regresyon")

        # Model tipine gÃ¶re uygun modeli seÃ§
        if model_type == 'classification':
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.metrics import classification_report, confusion_matrix

            model = RandomForestClassifier(n_estimators=100, random_state=random_state)
            model.fit(X_train, y_train)

            # Tahmin yap
            y_pred = model.predict(X_test)

            # Model performansÄ±nÄ± deÄŸerlendir
            print("\n=== SINIFLANDIRMA MODEL PERFORMANSI ===")
            print(classification_report(y_test, y_pred))

            # Confusion matrix
            cm = confusion_matrix(y_test, y_pred)
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
            plt.title('Confusion Matrix')
            plt.ylabel('GerÃ§ek DeÄŸerler')
            plt.xlabel('Tahmin Edilen DeÄŸerler')
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            plot_path = os.path.join(self.output_dir, f'confusion_matrix_{timestamp}.png')
            plt.tight_layout()
            plt.savefig(plot_path)
            plt.close()
            print(f"Confusion matrix gÃ¶rseli kaydedildi: {os.path.abspath(plot_path)}")

        else:  # regression
            from sklearn.ensemble import RandomForestRegressor
            from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

            model = RandomForestRegressor(n_estimators=100, random_state=random_state)
            model.fit(X_train, y_train)

            # Tahmin yap
            y_pred = model.predict(X_test)

            # Model performansÄ±nÄ± deÄŸerlendir
            print("\n=== REGRESYON MODEL PERFORMANSI ===")
            mse = mean_squared_error(y_test, y_pred)
            rmse = np.sqrt(mse)
            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)

            print(f"Mean Squared Error (MSE): {mse:.4f}")
            print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
            print(f"Mean Absolute Error (MAE): {mae:.4f}")
            print(f"RÂ² Score: {r2:.4f}")

            # GerÃ§ek vs Tahmin grafiÄŸi
            plt.figure(figsize=(10, 6))
            plt.scatter(y_test, y_pred, alpha=0.6)
            plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
            plt.xlabel('GerÃ§ek DeÄŸerler')
            plt.ylabel('Tahmin Edilen DeÄŸerler')
            plt.title('GerÃ§ek vs Tahmin DeÄŸerleri')
            plt.grid(True)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            plot_path = os.path.join(self.output_dir, f'regression_actual_vs_pred_{timestamp}.png')
            plt.tight_layout()
            plt.savefig(plot_path)
            plt.close()
            print(f"Regresyon gÃ¶rseli kaydedildi: {os.path.abspath(plot_path)}")

        self.pipeline_steps.append("Model eÄŸitimi tamamlandÄ±")
        return model, X_train, X_test, y_train, y_test

    def _build_sklearn_preprocessing(self, X: pd.DataFrame) -> ColumnTransformer:
        """
        Sklearn tabanlÄ± Ã¶n iÅŸleme pipeline'Ä± oluÅŸtur: KNNImputer + StandardScaler (sayÄ±sal),
        most_frequent imputation + OneHotEncoder (kategorik)
        """
        numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
        categorical_features = X.select_dtypes(exclude=[np.number]).columns.tolist()

        numeric_transformer = SkPipeline(steps=[
            ('imputer', KNNImputer(n_neighbors=5)),
            ('scaler', StandardScaler())
        ])

        categorical_transformer = SkPipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(handle_unknown='ignore'))
        ])

        preprocessor = ColumnTransformer(
            transformers=[
                ('num', numeric_transformer, numeric_features),
                ('cat', categorical_transformer, categorical_features)
            ], remainder='drop'
        )

        return preprocessor

    def train_with_sklearn_pipeline(self, X, y, test_size=0.2, random_state=42, model_type='auto'):
        """
        Sklearn ColumnTransformer + Pipeline kullanarak uÃ§tan uca eÄŸitim yap.
        SÄ±nÄ±flandÄ±rmada hedef deÄŸiÅŸken string ise LabelEncoder uygulanÄ±r.
        """
        print("=== SKLEARN PIPELINE Ä°LE MODEL EÄÄ°TÄ°MÄ° BAÅLADI ===")

        # Hedef tipini belirle ve gerekirse encode et
        y_encoded = y
        if model_type == 'auto':
            if y.dtype in ['object', 'bool'] or len(pd.Series(y).unique()) < 20:
                model_type = 'classification'
            else:
                model_type = 'regression'

        if model_type == 'classification' and y.dtype == 'object':
            self.target_label_encoder = LabelEncoder()
            y_encoded = self.target_label_encoder.fit_transform(y)
            print("Hedef deÄŸiÅŸken LabelEncoder ile dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼.")

        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, test_size=test_size, random_state=random_state
        )

        preprocessor = self._build_sklearn_preprocessing(X)

        if model_type == 'classification':
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.metrics import classification_report, confusion_matrix
            model = RandomForestClassifier(n_estimators=100, random_state=random_state)
        else:
            from sklearn.ensemble import RandomForestRegressor
            from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
            model = RandomForestRegressor(n_estimators=200, random_state=random_state)

        clf = SkPipeline(steps=[('preprocess', preprocessor), ('model', model)])
        clf.fit(X_train, y_train)

        y_pred = clf.predict(X_test)

        if model_type == 'classification':
            from sklearn.metrics import classification_report, confusion_matrix
            print("\n=== SINIFLANDIRMA (Sklearn Pipeline) ===")
            print(classification_report(y_test, y_pred))

            cm = confusion_matrix(y_test, y_pred)
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
            plt.title('Confusion Matrix (Sklearn Pipeline)')
            plt.ylabel('GerÃ§ek DeÄŸerler')
            plt.xlabel('Tahmin Edilen DeÄŸerler')
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            plot_path = os.path.join(self.output_dir, f'confusion_matrix_sklearn_{timestamp}.png')
            plt.tight_layout()
            plt.savefig(plot_path)
            plt.close()
            print(f"Confusion matrix gÃ¶rseli kaydedildi: {os.path.abspath(plot_path)}")
        else:
            from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
            print("\n=== REGRESYON (Sklearn Pipeline) ===")
            mse = mean_squared_error(y_test, y_pred)
            rmse = np.sqrt(mse)
            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            print(f"MSE: {mse:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f} | RÂ²: {r2:.4f}")

            plt.figure(figsize=(10, 6))
            plt.scatter(y_test, y_pred, alpha=0.6)
            plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--', lw=2)
            plt.xlabel('GerÃ§ek DeÄŸerler')
            plt.ylabel('Tahmin Edilen DeÄŸerler')
            plt.title('GerÃ§ek vs Tahmin (Sklearn Pipeline)')
            plt.grid(True)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            plot_path = os.path.join(self.output_dir, f'regression_actual_vs_pred_sklearn_{timestamp}.png')
            plt.tight_layout()
            plt.savefig(plot_path)
            plt.close()
            print(f"Regresyon gÃ¶rseli kaydedildi: {os.path.abspath(plot_path)}")

        self.pipeline_steps.append("Sklearn pipeline ile model eÄŸitimi tamamlandÄ±")
        return clf, X_train, X_test, y_train, y_test

    def generate_report(self, filepath: str = None):
        """
        EDA ve Ã¶n iÅŸleme adÄ±mlarÄ±nÄ±n kÄ±sa bir Markdown raporunu oluÅŸtur ve kaydet.
        """
        if filepath is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filepath = os.path.join(self.output_dir, f'eda_preprocessing_report_{timestamp}.md')

        lines = []
        lines.append('# EDA ve Ã–n Ä°ÅŸleme Raporu')
        lines.append('')
        if self.raw_data is not None:
            lines.append('## Veri Ã–zeti')
            lines.append(f"- Åekil: {self.raw_data.shape}")
            lines.append(f"- Bellek (MB): {self.raw_data.memory_usage(deep=True).sum() / 1024 ** 2:.2f}")
            lines.append('')

            missing = self.raw_data.isnull().sum()
            missing = missing[missing > 0]
            if not missing.empty:
                lines.append('## Eksik Veri')
                for col, val in missing.items():
                    pct = (val / len(self.raw_data)) * 100
                    lines.append(f"- {col}: {val} (%{pct:.2f})")
                lines.append('')

            numeric_cols = self.raw_data.select_dtypes(include=[np.number]).columns.tolist()
            if numeric_cols:
                lines.append('## SayÄ±sal SÃ¼tunlar (describe)')
                desc = self.raw_data[numeric_cols].describe().round(3)
                lines.append(desc.to_markdown())
                lines.append('')

        lines.append('## Pipeline AdÄ±mlarÄ±')
        for i, step in enumerate(self.pipeline_steps, 1):
            lines.append(f"{i}. {step}")

        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))

        print(f"Rapor kaydedildi: {os.path.abspath(filepath)}")
        return filepath

    def save_pipeline(self, filepath):
        """
        Pipeline'Ä± kaydet

        Args:
            filepath (str): KayÄ±t dosya yolu
        """
        pipeline_data = {
            'scaler': self.scaler,
            'label_encoders': self.label_encoders,
            'pipeline_steps': self.pipeline_steps,
            'processed_data_shape': self.processed_data.shape if self.processed_data is not None else None
        }

        joblib.dump(pipeline_data, filepath)
        print(f"Pipeline baÅŸarÄ±yla kaydedildi: {filepath}")

    def load_pipeline(self, filepath):
        """
        KaydedilmiÅŸ pipeline'Ä± yÃ¼kle

        Args:
            filepath (str): Pipeline dosya yolu
        """
        pipeline_data = joblib.load(filepath)

        self.scaler = pipeline_data['scaler']
        self.label_encoders = pipeline_data['label_encoders']
        self.pipeline_steps = pipeline_data['pipeline_steps']

        print(f"Pipeline baÅŸarÄ±yla yÃ¼klendi: {filepath}")
        print(f"Pipeline adÄ±mlarÄ±: {self.pipeline_steps}")

    def run_full_pipeline(self, target_column=None, save_pipeline=False, pipeline_path=None, model_type='auto',
                          include_eda=True):
        """
        TÃ¼m pipeline adÄ±mlarÄ±nÄ± Ã§alÄ±ÅŸtÄ±r

        Args:
            target_column (str): Hedef deÄŸiÅŸken sÃ¼tunu
            save_pipeline (bool): Pipeline'Ä± kaydet
            pipeline_path (str): Pipeline kayÄ±t yolu
            model_type (str): Model tipi ('auto', 'classification', 'regression')
            include_eda (bool): EDA gÃ¶rselleri ve rapor oluÅŸtur
        """
        print("ğŸš€ TAM PIPELINE BAÅLADI!")

        # 1. Veri yÃ¼kleme
        if self.raw_data is None:
            if self.data_path is None:
                print("âŒ Veri dosya yolu belirtilmemiÅŸ!")
                return None, None, None, None, None
            self.load_data()

        # 2. Veri keÅŸfi
        self.explore_data()

        # 3. EDA GÃ¶rselleri (opsiyonel)
        if include_eda:
            print("\nğŸ“Š EDA GÃ¶rselleri oluÅŸturuluyor...")
            self.visualize_data()

        # 4. Veri temizliÄŸi
        self.clean_data()

        # 5. Ã–zellik mÃ¼hendisliÄŸi
        # Hedef deÄŸiÅŸken kategorik ise korunacak sÃ¼tunlar listesine ekle
        preserve_columns = []
        if target_column and target_column in self.processed_data.columns:
            if self.processed_data[target_column].dtype == 'object':
                preserve_columns.append(target_column)
                print(f"Hedef deÄŸiÅŸken '{target_column}' korunuyor (one-hot encoding'den)")

        self.feature_engineering(preserve_columns=preserve_columns)

        # 6. Ã–zellik Ã¶lÃ§eklendirme
        self.scale_features()

        # 7. Modelleme hazÄ±rlÄ±ÄŸÄ±
        X, y = self.prepare_for_modeling(target_column)

        if X is not None and y is not None:
            # 8. Model eÄŸitimi
            model, X_train, X_test, y_train, y_test = self.train_model(X, y, model_type=model_type)

            # 9. EDA Raporu (opsiyonel)
            if include_eda:
                print("\nğŸ“‹ EDA raporu oluÅŸturuluyor...")
                self.generate_report()

            # 10. Pipeline kaydetme
            if save_pipeline and pipeline_path:
                self.save_pipeline(pipeline_path)

            print("\nğŸ‰ TAM PIPELINE BAÅARIYLA TAMAMLANDI!")
            print(f"Toplam adÄ±m sayÄ±sÄ±: {len(self.pipeline_steps)}")
            print("Pipeline adÄ±mlarÄ±:")
            for i, step in enumerate(self.pipeline_steps, 1):
                print(f"  {i}. {step}")

            return model, X_train, X_test, y_train, y_test

        return None, None, None, None, None

    def get_pipeline_summary(self):
        """
        Pipeline Ã¶zeti dÃ¶ndÃ¼r
        """
        summary = {
            'pipeline_steps': self.pipeline_steps,
            'raw_data_shape': self.raw_data.shape if self.raw_data is not None else None,
            'processed_data_shape': self.processed_data.shape if self.processed_data is not None else None,
            'total_steps': len(self.pipeline_steps)
        }

        return summary
